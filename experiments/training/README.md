# Training Experiments

Эта папка содержит конфигурации для обучения RL-алгоритмов и baseline эвристик.

## Структура

- `ppo/` - Конфигурации для обучения PPO (3 запуска)
- `sac/` - Конфигурации для обучения SAC (3 запуска)
- `td3/` - Конфигурации для обучения TD3 (3 запуска)
- `baseline/static/` - Конфигурации для static эвристики (3 запуска)
- `baseline/greedy_latency/` - Конфигурации для greedy_latency эвристики (3 запуска)

## Параметры обучения

**Реальное время:** 24 часа на запуск (фиксировано)

**Количество шагов (зависит от скорости алгоритма):**
- **SAC:** 1600 шагов за 24 часа (320 эпизодов)
- **TD3/PPO:** 960 шагов за 24 часа (192 эпизода)

**Профиль нагрузки:** Профиль A (полный суточный профиль)

## Запуск

Каждый алгоритм: 3 запуска параллельно на 3 VM = 1 день
Все алгоритмы: 3 дня (можно запускать последовательно или параллельно)

