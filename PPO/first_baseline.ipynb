{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from torch.distributions.categorical import Categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_size=64):\n",
    "        \"\"\"\n",
    "        obs_dim  : размерность вектора наблюдения\n",
    "        act_dim  : размерность (число возможных действий)\n",
    "        hidden_size: размер скрытых слоёв\n",
    "        \"\"\"\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Актер (Policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, act_dim)  # logits на act_dim действий\n",
    "        )\n",
    "        \n",
    "        # Критик (Value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)        # ценность состояния (скаляр)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        \"\"\"\n",
    "        Он просто есть, потому что мы обязаны привести реализацию этого класса \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_action_and_value(self, obs):\n",
    "        \"\"\"\n",
    "        Для заданного obs возвращаем:\n",
    "          - action (выбор из act_dim)\n",
    "          - log_prob(action)\n",
    "          - value (V(obs))\n",
    "          - распределение dist\n",
    "        \"\"\"\n",
    "        logits = self.actor(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        value = self.critic(obs).squeeze(-1)  # shape: [batch_size]\n",
    "        return action, dist.log_prob(action), dist, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, size, obs_dim):\n",
    "        \"\"\"\n",
    "        size   : сколько шагов опыта мы собираем перед обновлением (n_steps)\n",
    "        obs_dim: размер вектора наблюдения\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        \n",
    "        self.states = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros(size, dtype=np.int64)\n",
    "        self.rewards = np.zeros(size, dtype=np.float32)\n",
    "        self.dones = np.zeros(size, dtype=np.bool_)\n",
    "        \n",
    "        # log(pi(a|s)) — логарифм вероятности выбранного действия\n",
    "        self.log_probs = np.zeros(size, dtype=np.float32)\n",
    "        # V(s)\n",
    "        self.values = np.zeros(size, dtype=np.float32)\n",
    "        \n",
    "        # Для расчёта advantages\n",
    "        self.advantages = np.zeros(size, dtype=np.float32)\n",
    "        self.returns = np.zeros(size, dtype=np.float32)\n",
    "        \n",
    "        self.ptr = 0  # указатель на текущую позицию\n",
    "        \n",
    "    def store(self, state, action, reward, done, log_prob, value):\n",
    "        idx = self.ptr\n",
    "        self.states[idx] = state\n",
    "        self.actions[idx] = action\n",
    "        self.rewards[idx] = reward\n",
    "        self.dones[idx] = done\n",
    "        self.log_probs[idx] = log_prob\n",
    "        self.values[idx] = value\n",
    "        \n",
    "        self.ptr += 1\n",
    "        \n",
    "    def ready_for_update(self):\n",
    "        return self.ptr == self.size\n",
    "    \n",
    "    def compute_advantages(self, last_value, gamma=0.99, lam=0.95):\n",
    "        \"\"\"\n",
    "        last_value: V(s_{t+1}) — ценность последнего состояния (для GAE)\n",
    "        gamma     : discount factor\n",
    "        lam       : lambda для GAE\n",
    "        \"\"\"\n",
    "        # GAE лямбда (Generalized Advantage Estimation)\n",
    "        # a_t = delta_t + gamma*lam*delta_{t+1} + ... и т.д.\n",
    "        # delta_t = r_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "        \n",
    "        adv = 0.0\n",
    "        for i in reversed(range(self.size)):\n",
    "            if i == self.size - 1:\n",
    "                next_non_terminal = 1.0 - float(self.dones[i])\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - float(self.dones[i])\n",
    "                next_value = self.values[i + 1]\n",
    "            \n",
    "            delta = self.rewards[i] + gamma * next_value * next_non_terminal - self.values[i]\n",
    "            adv = delta + gamma * lam * next_non_terminal * adv\n",
    "            self.advantages[i] = adv\n",
    "        \n",
    "        self.returns = self.advantages + self.values\n",
    "    \n",
    "    def get(self, batch_size=None):\n",
    "        \"\"\"\n",
    "        Возвращает батчи данных (state, action, log_prob, return, advantage и т.д.)\n",
    "        Если batch_size=None, возвращаем всё сразу.\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.size\n",
    "        \n",
    "        indices = np.arange(self.size)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        start = 0\n",
    "        while start < self.size:\n",
    "            end = start + batch_size\n",
    "            yield (\n",
    "                self.states[indices[start:end]],\n",
    "                self.actions[indices[start:end]],\n",
    "                self.log_probs[indices[start:end]],\n",
    "                self.returns[indices[start:end]],\n",
    "                self.advantages[indices[start:end]]\n",
    "            )\n",
    "            start = end\n",
    "    \n",
    "    def reset(self):\n",
    "        self.ptr = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, \n",
    "                 obs_dim, \n",
    "                 act_dim,\n",
    "                 hidden_size=64,\n",
    "                 lr=3e-4,\n",
    "                 gamma=0.99,\n",
    "                 lam=0.95,\n",
    "                 clip_eps=0.2,\n",
    "                 ent_coef=0.0,\n",
    "                 vf_coef=0.5,\n",
    "                 n_steps=2048,\n",
    "                 batch_size=64,\n",
    "                 n_epochs=10,\n",
    "                 device='cpu'):\n",
    "        \"\"\"\n",
    "        obs_dim   : размерность наблюдения\n",
    "        act_dim   : размерность (количество дискретных действий)\n",
    "        hidden_size: размер скрытых слоёв в сети\n",
    "        lr        : learning rate\n",
    "        gamma     : discount factor\n",
    "        lam       : GAE-lambda\n",
    "        clip_eps  : epsilon в PPO objective (clip range)\n",
    "        ent_coef  : коэффициент при энтропии (необязательно)\n",
    "        vf_coef   : коэффициент при value loss\n",
    "        n_steps   : сколько шагов опыта собираем на каждом цикле обучения\n",
    "        batch_size: размер мини-батча при оптимизации\n",
    "        n_epochs  : число эпох обновления на каждом цикле\n",
    "        device    : 'cpu' или 'cuda'\n",
    "        \"\"\"\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_eps = clip_eps\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        \n",
    "        self.n_steps = n_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        # Модель\n",
    "        self.model = ActorCritic(obs_dim, act_dim, hidden_size).to(device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # Буфер\n",
    "        self.buffer = RolloutBuffer(n_steps, obs_dim)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Выбираем действие (action) из политики в режиме training.\n",
    "        state: np.array(obs_dim) — одномерный вектор наблюдения.\n",
    "        Возвращаем: action, log_prob, value\n",
    "        \"\"\"\n",
    "        state_t = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "        # batch размером [1, obs_dim]\n",
    "        state_t = state_t.unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action, log_prob, dist, value = self.model.get_action_and_value(state_t)\n",
    "        \n",
    "        return (action.item(), \n",
    "                log_prob.cpu().item(),\n",
    "                value.cpu().item())\n",
    "    \n",
    "    def collect_trajectories(self, env):\n",
    "        \"\"\"\n",
    "        Собираем n_steps переходов в буфер (self.buffer).\n",
    "        Затем считаем GAE.\n",
    "        \"\"\"\n",
    "        self.buffer.reset()\n",
    "        \n",
    "        # Начинаем с нового эпизода\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(self.n_steps):\n",
    "            action, log_prob, value = self.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            self.buffer.store(\n",
    "                state, action, reward, done, log_prob, value\n",
    "            )\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                # начинаем новый эпизод, если эпизод закончился\n",
    "                state = env.reset()\n",
    "        \n",
    "        # Получим V(s_{t+1}) для последнего состояния (или 0, если done)\n",
    "        # Здесь берём последнее состояние из буфера (или текущее state)\n",
    "        if not done:\n",
    "            with torch.no_grad():\n",
    "                next_state_t = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "                next_state_t = next_state_t.unsqueeze(0)\n",
    "                _, _, _, last_value = self.model.get_action_and_value(next_state_t)\n",
    "                last_value = last_value.cpu().item()\n",
    "        else:\n",
    "            last_value = 0.0\n",
    "        \n",
    "        # Считаем advantages\n",
    "        self.buffer.compute_advantages(last_value, gamma=self.gamma, lam=self.lam)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Обновляем параметры Actor-Critic на основе данных в self.buffer.\n",
    "        Выполняем n_epochs проходов по батчам.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_epochs):\n",
    "            for states_b, actions_b, old_log_probs_b, returns_b, advantages_b in self.buffer.get(batch_size=self.batch_size):\n",
    "                states_t = torch.tensor(states_b, dtype=torch.float32).to(self.device)\n",
    "                actions_t = torch.tensor(actions_b, dtype=torch.long).to(self.device)\n",
    "                old_log_probs_t = torch.tensor(old_log_probs_b, dtype=torch.float32).to(self.device)\n",
    "                returns_t = torch.tensor(returns_b, dtype=torch.float32).to(self.device)\n",
    "                advantages_t = torch.tensor(advantages_b, dtype=torch.float32).to(self.device)\n",
    "                \n",
    "                # Нормировка advantages (часто помогает)\n",
    "                advantages_t = (advantages_t - advantages_t.mean()) / (advantages_t.std() + 1e-8)\n",
    "                \n",
    "                # Получаем новое распределение, значения\n",
    "                logits = self.model.actor(states_t)\n",
    "                dist = Categorical(logits=logits)\n",
    "                log_probs_t = dist.log_prob(actions_t)\n",
    "                \n",
    "                values_t = self.model.critic(states_t).squeeze(-1)\n",
    "                \n",
    "                # Вычислим ratio = exp(new_log_prob - old_log_prob)\n",
    "                ratio = torch.exp(log_probs_t - old_log_probs_t)\n",
    "                \n",
    "                # L_clip = min(ratio * A, clip(ratio, 1-eps, 1+eps) * A)\n",
    "                clip_adv = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * advantages_t\n",
    "                loss_policy = -torch.mean(torch.min(ratio * advantages_t, clip_adv))\n",
    "                \n",
    "                # Entropy (для улучшения исследования)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Value loss\n",
    "                loss_value = nn.functional.mse_loss(values_t, returns_t)\n",
    "                \n",
    "                # Финальный лосс\n",
    "                loss = loss_policy + self.vf_coef * loss_value - self.ent_coef * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def train(self, env, total_timesteps=1_000_000, log_interval=1000):\n",
    "        \"\"\"\n",
    "        Цикл обучения: собираем n_steps опыта -> update -> повторяем\n",
    "        total_timesteps: общее кол-во шагов\n",
    "        log_interval   : как часто печатать лог\n",
    "        \"\"\"\n",
    "        timesteps_done = 0\n",
    "        iteration = 0\n",
    "        \n",
    "        while timesteps_done < total_timesteps:\n",
    "            self.collect_trajectories(env)\n",
    "            self.update()\n",
    "            \n",
    "            timesteps_done += self.n_steps\n",
    "            iteration += 1\n",
    "            \n",
    "            if iteration % (log_interval // self.n_steps) == 0:\n",
    "                print(f\"Iteration={iteration}, timesteps_done={timesteps_done}\")\n",
    "                \n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.model.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(env, ppo_agent, n_episodes=5):\n",
    "    \"\"\"\n",
    "    Запускаем политику ppo_agent (без обучения)\n",
    "    на n_episodes в среде env.\n",
    "    Возвращаем среднюю награду и среднюю длину эпизода.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _, _ = ppo_agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "    \n",
    "    mean_reward = np.mean(total_rewards)\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class TestBanditEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Bandit-среда c дискретными действиями.\n",
    "    - Есть N действий\n",
    "    - При каждом эпизоде среда завершается за 1 шаг (done = True)\n",
    "    - Награда = случайная, но у одного действия мат.ожидание больше\n",
    "    \"\"\"\n",
    "    def __init__(self, n_actions=3, best_action=1, rng_seed=42):\n",
    "        super(TestBanditEnv, self).__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.best_action = best_action\n",
    "        self.rng = np.random.RandomState(rng_seed)\n",
    "        \n",
    "        # Дискретное пространство действий: 0..n_actions-1\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # Пускай наблюдение просто пустое (shape=(1,)) или нулевой вектор;\n",
    "        # т.к. это бандит — нет особой зависимости от \"состояния\".\n",
    "        # Но Gym требует какое-то observation_space.\n",
    "        self.observation_space = spaces.Box(low=-1., high=1., shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        # Возвращаем \"пустое\" наблюдение, например, [0.]\n",
    "        return np.array([0.0], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        # Вознаграждение: у \"best_action\" мат.ожидание = +1.0,\n",
    "        # у остальных — 0.0.\n",
    "        # Добавим небольшой разброс.\n",
    "        if action == self.best_action:\n",
    "            reward = 1.0 + 0.1 * self.rng.randn()\n",
    "        else:\n",
    "            reward = 0.0 + 0.1 * self.rng.randn()\n",
    "        \n",
    "        # Одношаговый эпизод: сразу done = True\n",
    "        done = True\n",
    "        info = {}\n",
    "        \n",
    "        # Возвращаем фиктивное наблюдение\n",
    "        obs = np.array([0.0], dtype=np.float32)\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class NodeReallocationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "      - N нод (каждая имеет лимиты по CPU и памяти).\n",
    "      - M пользователей (каждый требует CPU_i, MEM_i).\n",
    "      - За шаг можно (пере)назначить ровно одного пользователя (user_idx) на одну ноду (node_idx).\n",
    "      - Эпизод завершается, когда либо все пользователи назначены, либо достигли max_steps.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_nodes=3,\n",
    "                 num_users=5,\n",
    "                 max_steps=20,\n",
    "                 cpu_caps=None,\n",
    "                 mem_caps=None,\n",
    "                 user_cpu_req=None,\n",
    "                 user_mem_req=None,\n",
    "                 overload_penalty=100.0):\n",
    "        super(NodeReallocationEnv, self).__init__()\n",
    "        \n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_users = num_users\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        \n",
    "        if cpu_caps is None:\n",
    "            cpu_caps = np.random.uniform(3.0, 5.0, size=num_nodes)\n",
    "        if mem_caps is None:\n",
    "            mem_caps = np.random.uniform(8.0, 10.0, size=num_nodes)\n",
    "        self.cpu_caps = np.array(cpu_caps, dtype=np.float32)\n",
    "        self.mem_caps = np.array(mem_caps, dtype=np.float32)\n",
    "        \n",
    "        \n",
    "        if user_cpu_req is None:\n",
    "            user_cpu_req = np.random.uniform(0.5, 1.5, size=num_users)\n",
    "        if user_mem_req is None:\n",
    "            user_mem_req = np.random.uniform(1.0, 2.5, size=num_users)\n",
    "        self.user_cpu_req = np.array(user_cpu_req, dtype=np.float32)\n",
    "        self.user_mem_req = np.array(user_mem_req, dtype=np.float32)\n",
    "        \n",
    "        # Назначение: для каждого пользователя сохраняем индекс ноды (или -1, если не назначен)\n",
    "        self.assignment = np.full(shape=(num_users,), fill_value=-1, dtype=np.int32)\n",
    "        \n",
    "        # Штраф за превышение лимитов\n",
    "        self.overload_penalty = overload_penalty\n",
    "        \n",
    "        # Количество сделанных шагов\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # ACTION SPACE\n",
    "        # Одно действие = (пользователь, нода).\n",
    "        # Кодируем это одним числом: action in [0..(num_users*num_nodes - 1)]\n",
    "        # user_idx = action // num_nodes\n",
    "        # node_idx = action % num_nodes\n",
    "        self.action_space = spaces.Discrete(num_users * num_nodes)\n",
    "        \n",
    "        # OBSERVATION SPACE\n",
    "        #  - [CPU usage ratio по всем нодам]\n",
    "        #  - [MEM usage ratio по всем нодам]\n",
    "        #  - [assignment каждого пользователя], нормируем, чтобы лежало ~ в 0..1\n",
    "        #    (если -1, то кодируем отрицательным значением)\n",
    "        obs_dim = num_nodes * 2 + num_users\n",
    "        low = np.zeros(obs_dim, dtype=np.float32)\n",
    "        high = np.ones(obs_dim, dtype=np.float32) * 2.0  # запас\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(obs_dim,), dtype=np.float32)\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Сбрасываем окружение к начальному состоянию:\n",
    "         - никто не назначен (assignment = -1)\n",
    "         - обнуляем счётчик шагов.\n",
    "        \"\"\"\n",
    "        self.assignment[:] = -1\n",
    "        self.current_step = 0\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Выполняем 1 шаг:\n",
    "          - Раскодируем (user_idx, node_idx)\n",
    "          - Переназначаем пользователя\n",
    "          - Считаем загруженность, задержку и штрафы\n",
    "          - Проверяем, не завершился ли эпизод\n",
    "        \"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        user_idx = action // self.num_nodes\n",
    "        node_idx = action % self.num_nodes\n",
    "        \n",
    "        # Переназначаем данного пользователя\n",
    "        self.assignment[user_idx] = node_idx\n",
    "        \n",
    "        # Считаем текущее использование CPU/Memory\n",
    "        cpu_used, mem_used = self._get_usage()\n",
    "        \n",
    "        # Считаем delay: суммарная загрузка (CPU ratio + MEM ratio) по всем нодам\n",
    "        # и штраф за превышение\n",
    "        over_capacity = False\n",
    "        delay = 0.0\n",
    "        for n in range(self.num_nodes):\n",
    "            # Проверка превышения\n",
    "            if cpu_used[n] > self.cpu_caps[n] or mem_used[n] > self.mem_caps[n]:\n",
    "                over_capacity = True\n",
    "            ratio_cpu = cpu_used[n] / (self.cpu_caps[n] + 1e-8)\n",
    "            ratio_mem = mem_used[n] / (self.mem_caps[n] + 1e-8)\n",
    "            delay += (ratio_cpu + ratio_mem)\n",
    "        \n",
    "        # reward = -delay (+ штраф, если overload)\n",
    "        reward = -delay\n",
    "        if over_capacity:\n",
    "            reward -= self.overload_penalty\n",
    "        \n",
    "        # Условие завершения\n",
    "        # 1) Если все пользователи назначены (нет -1)\n",
    "        all_assigned = np.all(self.assignment != -1)\n",
    "        \n",
    "        # 2) Если достигли max_steps\n",
    "        time_exceeded = (self.current_step >= self.max_steps)\n",
    "        \n",
    "        done = bool(all_assigned or time_exceeded)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        info = {\n",
    "            \"delay\": delay,\n",
    "            \"over_capacity\": over_capacity,\n",
    "            \"all_assigned\": all_assigned\n",
    "        }\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def _get_usage(self):\n",
    "        \"\"\"\n",
    "        Возвращает (cpu_used, mem_used) по каждой ноде.\n",
    "        \"\"\"\n",
    "        cpu_used = np.zeros(self.num_nodes, dtype=np.float32)\n",
    "        mem_used = np.zeros(self.num_nodes, dtype=np.float32)\n",
    "        \n",
    "        for u in range(self.num_users):\n",
    "            n = self.assignment[u]\n",
    "            if n >= 0:  # пользователь назначен\n",
    "                cpu_used[n] += self.user_cpu_req[u]\n",
    "                mem_used[n] += self.user_mem_req[u]\n",
    "        \n",
    "        return cpu_used, mem_used\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Формируем наблюдение.\n",
    "        - cpu ratio для каждой ноды\n",
    "        - mem ratio для каждой ноды\n",
    "        - \"normalized assignment\" для каждого пользователя\n",
    "          (если assignment[u] = -1 => -1/(num_nodes-1); иначе => node/(num_nodes-1))\n",
    "        \"\"\"\n",
    "        cpu_used, mem_used = self._get_usage()\n",
    "        cpu_ratio = cpu_used / (self.cpu_caps + 1e-8)\n",
    "        mem_ratio = mem_used / (self.mem_caps + 1e-8)\n",
    "        \n",
    "        # Нормируем назначение\n",
    "        assign_norm = []\n",
    "        for a in self.assignment:\n",
    "            if a < 0:\n",
    "                val = -1.0 / max((self.num_nodes - 1), 1)\n",
    "            else:\n",
    "                val = a / max((self.num_nodes - 1), 1)\n",
    "            assign_norm.append(val)\n",
    "        \n",
    "        obs = np.concatenate([cpu_ratio, mem_ratio, assign_norm], dtype=np.float32)\n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        cpu_used, mem_used = self._get_usage()\n",
    "        print(f\"Step={self.current_step}\")\n",
    "        for n in range(self.num_nodes):\n",
    "            print(f\" Node {n}: CPU used {cpu_used[n]:.2f}/{self.cpu_caps[n]:.2f},\"\n",
    "                  f\" Mem used {mem_used[n]:.2f}/{self.mem_caps[n]:.2f}\")\n",
    "        print(\" Assignment:\", self.assignment)\n",
    "        print(\"----------\")\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1, timesteps_done=2048\n",
      "Iteration=2, timesteps_done=4096\n",
      "Iteration=3, timesteps_done=6144\n",
      "Iteration=4, timesteps_done=8192\n",
      "Iteration=5, timesteps_done=10240\n",
      "Step=12\n",
      " Node 0: CPU used 2.80/5.00, Mem used 2.20/10.00\n",
      " Node 1: CPU used 3.30/5.00, Mem used 5.30/9.00\n",
      " Node 2: CPU used 0.70/4.00, Mem used 1.00/8.00\n",
      " Assignment: [1 1 0 1 2]\n",
      "----------\n",
      "Cumulative reward after test episode: -20.635555\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    # env = TestBanditEnv(n_actions=3, best_action=1, rng_seed=123)\n",
    "    env = NodeReallocationEnv(\n",
    "        num_nodes=3,\n",
    "        num_users=5,\n",
    "        max_steps=20,\n",
    "        cpu_caps=[5.0, 5.0, 4.0],\n",
    "        mem_caps=[10.0, 9.0, 8.0],\n",
    "        user_cpu_req=[1.0, 1.1, 2.8, 1.2, 0.7],\n",
    "        user_mem_req=[2.0, 1.5, 2.2, 1.8, 1.0],\n",
    "        overload_penalty=100.0,\n",
    "    )\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n  \n",
    "    \n",
    "    # Создаём агент PPO\n",
    "    ppo_agent = PPO(\n",
    "        obs_dim=obs_dim,\n",
    "        act_dim=act_dim,\n",
    "        hidden_size=64,\n",
    "        lr=3e-4,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        clip_eps=0.2,\n",
    "        ent_coef=0.0,\n",
    "        vf_coef=0.5,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        device='cpu'\n",
    "    )\n",
    "    \n",
    "    # Запускаем обучение\n",
    "    ppo_agent.train(env, total_timesteps=10_000, log_interval=2048)\n",
    "\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0.0\n",
    "    while not done:\n",
    "        action, _, _ = ppo_agent.select_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        cum_reward += reward\n",
    "        state = next_state\n",
    "    \n",
    "    env.render()\n",
    "    print(\"Cumulative reward after test episode:\", cum_reward)\n",
    "    # # Сохраняем\n",
    "    # ppo_agent.save(\"./ppo_lwme_model.pth\")\n",
    "    \n",
    "    # # Валидация\n",
    "    # avg_reward = validate(env, ppo_agent, n_episodes=5)\n",
    "    # print(\"Validation reward:\", avg_reward)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
